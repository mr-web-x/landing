import os
from bs4 import BeautifulSoup
from collections import defaultdict
import re
from datetime import datetime
import xml.etree.ElementTree as ET

BLOG_DIR = "blog"
INDEX_FILE = "index.html"
ACCORDION_ID = "blog-list"
BLOG_SECTION_ID = "blog"
SITEMAP_FILE = "sitemap.xml"
SITE_URL = "https://fastcredit.sk"

def extract_section_content(html_content, section_id):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ–∫—Ü–∏–∏ –ø–æ –µ—ë ID"""
    soup = BeautifulSoup(html_content, "html.parser")
    section = soup.find("section", {"id": section_id})
    return str(section).strip() if section else ""

def extract_article_info(filepath):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ –∏–∑ HTML —Ñ–∞–π–ª–∞"""
    # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    filename = os.path.basename(filepath)
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å "–ü–†–ò–ú–ï–†"
    if filename.startswith("–ü–†–ò–ú–ï–†"):
        return None
    
    with open(filepath, "r", encoding="utf-8") as file:
        soup = BeautifulSoup(file, "html.parser")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ h1
        h1 = soup.find("h1")
        title = h1.text.strip() if h1 else ""
        
        # –ï—Å–ª–∏ h1 –ø—É—Å—Ç–æ–π, –±–µ—Ä–µ–º –∏–∑ title
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.text.strip()
                # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å —Å–∞–π—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
                title = title.split(" | ")[0].strip()
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ—Å–ª–µ h1 –∏–ª–∏ meta description)
        meta_desc = soup.find("meta", {"name": "description"})
        description = ""
        
        if meta_desc and meta_desc.get("content"):
            description = meta_desc["content"].strip()
        else:
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ—Å–ª–µ h1
            if h1:
                next_p = h1.find_next_sibling("p")
                if next_p:
                    description = next_p.text.strip()[:200] + "..." if len(next_p.text) > 200 else next_p.text.strip()
        
        return {
            "title": title,
            "description": description,
            "filename": filename,
            "href": f"/blog/{filename}"
        }

def generate_blog_slider_html(articles):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–∞ –±–ª–æ–≥–∞"""
    slides_html = ""
    
    for article in articles:
        slide = f'''                <div class="swiper-slide">
                  <a href="{article['href']}" class="blog-card">
                    <img
                      src="/assets/blog/b1.webp"
                      alt="articles"
                      loading="lazy"
                    />
                    <h3>{article['title']}</h3>
                    <p>{article['description']}</p>
                    <div class="blog-card-bottom">
                      <span>Podrobnosti ‚Üí</span>
                    </div>
                  </a>
                </div>
'''
        slides_html += slide
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é —Å–µ–∫—Ü–∏—é
    section_html = f'''      <section id="blog" class="blog-home">
        <div class="container">
          <h2>N√°≈° blog</h2>
          <div class="blog-home__wrapper">
            <div class="swiper blogSwiper">
              <div class="swiper-wrapper">
{slides_html.rstrip()}
              </div>
              <div class="swiper-button-next"></div>
              <div class="swiper-button-prev"></div>
              <div class="swiper-pagination"></div>
            </div>
          </div>
        </div>
      </section>'''
    
    return section_html

def find_unique_section():
    """–ù–∞—Ö–æ–¥–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–µ–∫—Ü–∏—é blog-list —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –±–ª–æ–≥–∞"""
    content_map = defaultdict(list)
    
    for filename in os.listdir(BLOG_DIR):
        if filename.endswith(".html"):
            path = os.path.join(BLOG_DIR, filename)
            with open(path, "r", encoding="utf-8") as file:
                html = file.read()
                section_html = extract_section_content(html, ACCORDION_ID)
                if section_html:
                    content_map[section_html].append(filename)

    if len(content_map) == 1:
        print("‚ö†Ô∏è –í—Å–µ —Å–µ–∫—Ü–∏–∏ blog-list –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ ‚Äî –Ω–µ—á–µ–≥–æ –æ–±–Ω–æ–≤–ª—è—Ç—å.")
        return None

    for content, files in content_map.items():
        if len(files) == 1:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è blog-list –≤ —Ñ–∞–π–ª–µ: {files[0]}")
            return content

    print("‚ùå –ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–∏ blog-list ‚Äî –ª–∏–±–æ –≤—Å–µ —Ä–∞–∑–Ω—ã–µ, –ª–∏–±–æ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫—É —Ä–∞–∑.")
    return None

def update_all_articles(new_section_html):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é blog-list –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö –±–ª–æ–≥–∞"""
    updated_count = 0
    
    for filename in os.listdir(BLOG_DIR):
        if filename.endswith(".html"):
            path = os.path.join(BLOG_DIR, filename)
            with open(path, "r", encoding="utf-8") as file:
                soup = BeautifulSoup(file, "html.parser")
            
            old_section = soup.find("section", {"id": ACCORDION_ID})
            if old_section:
                new_section_soup = BeautifulSoup(new_section_html, "html.parser")
                old_section.replace_with(new_section_soup)
                
                with open(path, "w", encoding="utf-8") as file:
                    file.write(str(soup))
                updated_count += 1
    
    return updated_count

def update_index_blog_section():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é –±–ª–æ–≥–∞ –≤ index.html –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π"""
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö —Å—Ç–∞—Ç—å—è—Ö
    articles = []
    
    for filename in sorted(os.listdir(BLOG_DIR)):
        if filename.endswith(".html"):
            path = os.path.join(BLOG_DIR, filename)
            article_info = extract_article_info(path)
            if article_info:  # None –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "–ü–†–ò–ú–ï–†"
                articles.append(article_info)
    
    if not articles:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ index.html")
        return False
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π HTML –¥–ª—è —Å–µ–∫—Ü–∏–∏ –±–ª–æ–≥–∞
    new_blog_section = generate_blog_slider_html(articles)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º index.html
    try:
        with open(INDEX_FILE, "r", encoding="utf-8") as file:
            soup = BeautifulSoup(file, "html.parser")
        
        old_section = soup.find("section", {"id": BLOG_SECTION_ID})
        if old_section:
            new_section_soup = BeautifulSoup(new_blog_section, "html.parser")
            old_section.replace_with(new_section_soup)
            
            with open(INDEX_FILE, "w", encoding="utf-8") as file:
                file.write(str(soup))
            
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å–µ–∫—Ü–∏—è –±–ª–æ–≥–∞ –≤ index.html ({len(articles)} —Å—Ç–∞—Ç–µ–π)")
            return True
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–µ–∫—Ü–∏—è –±–ª–æ–≥–∞ –≤ index.html")
            return False
            
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {INDEX_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ index.html: {e}")
        return False

def update_sitemap():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç sitemap.xml, –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏"""
    try:
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º namespace –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å XML
        ET.register_namespace('', 'http://www.sitemaps.org/schemas/sitemap/0.9')
        ET.register_namespace('xsi', 'http://www.w3.org/2001/XMLSchema-instance')
        
        # –ü–∞—Ä—Å–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π sitemap
        tree = ET.parse(SITEMAP_FILE)
        root = tree.getroot()
        
        # –ü–æ–ª—É—á–∞–µ–º namespace
        ns = {'': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ URL –∏–∑ sitemap
        existing_urls = set()
        for url in root.findall('url', ns):
            loc = url.find('loc', ns)
            if loc is not None and loc.text:
                existing_urls.add(loc.text.strip())
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO
        current_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+00:00')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –≤ –ø–∞–ø–∫–µ blog
        new_articles_count = 0
        
        for filename in sorted(os.listdir(BLOG_DIR)):
            if filename.endswith(".html") and not filename.startswith("–ü–†–ò–ú–ï–†"):
                article_url = f"{SITE_URL}/blog/{filename}"
                
                # –ï—Å–ª–∏ URL –µ—â–µ –Ω–µ—Ç –≤ sitemap, –¥–æ–±–∞–≤–ª—è–µ–º
                if article_url not in existing_urls:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç url
                    url_elem = ET.SubElement(root, 'url')
                    
                    loc_elem = ET.SubElement(url_elem, 'loc')
                    loc_elem.text = f" {article_url}"  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª –≤ –Ω–∞—á–∞–ª–µ –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ
                    
                    lastmod_elem = ET.SubElement(url_elem, 'lastmod')
                    lastmod_elem.text = current_date
                    
                    priority_elem = ET.SubElement(url_elem, 'priority')
                    priority_elem.text = '0.80'
                    
                    new_articles_count += 1
                    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –≤ sitemap: {filename}")
        
        if new_articles_count > 0:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º XML —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏
            indent_xml(root)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π sitemap
            tree.write(SITEMAP_FILE, encoding='UTF-8', xml_declaration=True)
            print(f"‚úÖ Sitemap –æ–±–Ω–æ–≤–ª–µ–Ω! –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö URL: {new_articles_count}")
        else:
            print("‚ÑπÔ∏è –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        return True
        
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {SITEMAP_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ sitemap: {e}")
        return False

def indent_xml(elem, level=0):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç—Å—Ç—É–ø—ã –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è XML"""
    i = "\n" + level * "  "
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + "  "
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for child in elem:
            indent_xml(child, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i

if __name__ == "__main__":
    print("="*50)
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –±–ª–æ–≥–∞")
    print("="*50)
    
    # –≠—Ç–∞–ø 1: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π blog-list
    print("\nüìã –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–∏ <section id='blog-list'>...")
    new_accordion_html = find_unique_section()
    
    if new_accordion_html:
        print("üîÑ –û–±–Ω–æ–≤–ª—è—é –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –Ω–æ–≤–æ–π —Å–µ–∫—Ü–∏–µ–π blog-list...")
        updated = update_all_articles(new_accordion_html)
        print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated} —Ñ–∞–π–ª–æ–≤!")
    
    # –≠—Ç–∞–ø 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–∞–π–¥–µ—Ä–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    print("\nüìã –≠—Ç–∞–ø 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–∞–π–¥–µ—Ä–∞ –±–ª–æ–≥–∞ –≤ index.html...")
    if update_index_blog_section():
        print("üéâ –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")
    
    # –≠—Ç–∞–ø 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ sitemap.xml
    print("\nüìã –≠—Ç–∞–ø 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ sitemap.xml...")
    if update_sitemap():
        print("üó∫Ô∏è –ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")
    
    print("\n‚ú® –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")